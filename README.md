# Enron-Email-Classification
Aim: To model an algorithm which can classify emails into six categories based on the content it contains.

Data Preprocessing:
1) The data given is in form of .txt file and its corresponding class in form of .cats file in 8 folders. A loop is run to copy all the content present in the .txt file and its corresponding class to list of lists.
2) A DataFrame is made using the list of lists, some of emails belong to multiple categories as well. Hence such emails are filtered and duplicate data has been added to the dataset.
3) Unwanted data has been removed from the dataframe and only file name, Email body and its Label were retained.
4) Email body must be filtered so that predictions can be better, hence initially, all text data is converted to .lower() format and stopwords, numerics, https, email accounts are removed from the data.
5) Labels which are in float formats are converted to int formats. Any spacing before or after texts are removed.
6) To reduce text complexity, all the words are cut to their base stems using PorterStemmer and final dataset is prepared which contains only file name, from, subject, email body and label.

Data Analysis:
1) A histogram is plotted to check the count of different categories of emails present in the dataset. It was found that 855 first label and 533 fourth label emails are in data. Hence it can be considered as imbalanced data as 70% of data belongs to 2 categories and remaining 4 categories belong to 30% of the data.
2) Of all email senders, 1038 emails were sent by Steven Kean and of them, 422 belong to first category and 366 belong to fourth category.
3) Wordclouds were plotted to check what types of words are present as common ones in different categories of emails.

Modelling:
1) Countvectorizer model: A continuous bag of words model is generated by taking set of all words present in the dataset and indexed in ascending alphabetical order. Top 1000 most frequent words were taken as max features variable for modelling the count vectorizer model.
2) Tdidf Model: Term Frequency Inverse Document Frequency model is generated and max features were taken as 1000.
3) Word2Vec Model: An Embedding matrix is created from all available words present in the data and a Word2Vec model is generated from sparse matrix which is 200 dimensional vector. Hence entire relation of word wrt all other words is determined by these 200 values only.
4) Logistic Regression: As an initial task, Logistic regression modelling is done on above 3 data values. As it is multiclass classification, softmax classifier is used.
5) Naïve Bayes: 3 types of Naïve Bayes ( GaussianNB(), BernoulliNB(), MultinomialNB()) are modelled using 3 data values. Performance metrics are drawn to evaluate performance of these models.
6) Random Forest: Random Forest model is trained for above 3 data values. Performance metrics are drawn to evaluate performance of these models.
7) K Nearest Neighbours: K Nearest Neighbours model is trained for above 3 data values. Neighbours value is changed for better performance, n=2 gave better results compared to other values. Performance metrics are drawn to evaluate performance of these models.
8) Artifical Neural Network: ANN is modelled for above 3 models and trained for 100 epochs. Performance metrics are drawn to evaluate performance of these models.
9) RNN - LSTM Network: From Embedding Matrix, padded sequences of maximum length 200 are generated and fed into neural network and trained for 40 epochs. Performance metrics are drawn to evaluate performance of these models.

Result:
1) Logistic Regression: Of 3 models made using logistic regression, achieved highest accuracy of 67.4% using Word2Vec model.
2) Naïve Bayes: Of 3 types of Naïve Bayes ( GaussianNB(), BernoulliNB(), MultinomialNB()) and modelling using 3 data values, of total 9 analysis, highest accuracy of 61.13% is achieved using Word2Vec model.
3) Random Forest: Of 3 types of Random Forest models, highest accuracy of 68.37% is achieved using Word2Vec model.
4) K Nearest Neighbours: Of 3 types of K Nearest Neighbours, highest accuracy of 63.855% is achieved using Tfidf model.
5) Artifical Neural Network: Of 3 types of Artifical Neural Network models, highest accuracy of 65.6% is achieved using Word2Vec model.
6) RNN – LSTM Network: LSTM network 256 LSTM units considered, gave highest accurary of 66%.
The above mentioned accuracies are in ideal case and performance of each algorithm can be increased by Hyper Parameter Tuning respective parameters
As accuracy of prediction for each algorithm is less, majority voting of all 6 algorithm is taken (Bagging) to make prediction for test data.

Analysis of Results:
1) Maximum accuracy reached by above algorithms is 70% and it can be increased by proper hyper parameter tuning.
2) The data seems to be imbalanced as majority of labels are in category 1 and 4. Hence performance is not accurate.
3) Of categories 1 and 4, similarity between the emails is more. Hence misclassification between these 2 categories are high when compared to other categories.
